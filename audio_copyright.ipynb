{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Samyak\\Documents\\projects\\Larsen hackathon\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Samyak\\Documents\\projects\\Larsen hackathon\\.venv\\Lib\\site-packages\\transformers\\configuration_utils.py:306: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForSequenceClassification\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load Wav2Vec2 processor\n",
    "model_name = \"facebook/wav2vec2-base\"\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "\n",
    "def load_audio(file_path, target_length=160000):\n",
    "    \"\"\"\n",
    "    Loads an audio file and ensures a fixed length by padding or truncating.\n",
    "    \n",
    "    Parameters:\n",
    "        file_path (str): Path to the audio file.\n",
    "        target_length (int): Fixed length for all audio samples (e.g., 10s at 16kHz = 160000 samples).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Fixed-length waveform tensor.\n",
    "    \"\"\"\n",
    "    waveform, sample_rate = torchaudio.load(file_path)\n",
    "    resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
    "    waveform = resampler(waveform)  # Convert to 16kHz\n",
    "\n",
    "    # Ensure waveform is mono (1 channel)\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)\n",
    "\n",
    "    # Pad or truncate to target length\n",
    "    if waveform.shape[1] < target_length:\n",
    "        pad_amount = target_length - waveform.shape[1]\n",
    "        waveform = torch.nn.functional.pad(waveform, (0, pad_amount))\n",
    "    else:\n",
    "        waveform = waveform[:, :target_length]  # Truncate\n",
    "\n",
    "    return waveform.squeeze(0)\n",
    "\n",
    "\n",
    "# Load dataset labels\n",
    "df = pd.read_csv(r\"dataset\\labels.csv\")\n",
    "\n",
    "# Prepare dataset\n",
    "train_audio = []\n",
    "train_labels = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    file_path = os.path.join(\"dataset/train\", row[\"File Name\"])\n",
    "    waveform = load_audio(file_path)\n",
    "    train_audio.append(waveform)\n",
    "    train_labels.append(row[\"Label\"])\n",
    "\n",
    "# Convert to tensors\n",
    "train_audio = torch.stack(train_audio)\n",
    "train_labels = torch.tensor(train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Samyak\\Documents\\projects\\Larsen hackathon\\.venv\\Lib\\site-packages\\transformers\\configuration_utils.py:306: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\Samyak\\Documents\\projects\\Larsen hackathon\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]c:\\Users\\Samyak\\Documents\\projects\\Larsen hackathon\\.venv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "                                              \n",
      "  7%|â–‹         | 1/15 [00:15<03:05, 13.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5930120348930359, 'eval_runtime': 2.2282, 'eval_samples_per_second': 2.244, 'eval_steps_per_second': 0.449, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                              \n",
      " 13%|â–ˆâ–Ž        | 2/15 [00:29<02:57, 13.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.49818211793899536, 'eval_runtime': 2.2342, 'eval_samples_per_second': 2.238, 'eval_steps_per_second': 0.448, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                              \n",
      " 20%|â–ˆâ–ˆ        | 3/15 [00:43<02:44, 13.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4333652853965759, 'eval_runtime': 2.3194, 'eval_samples_per_second': 2.156, 'eval_steps_per_second': 0.431, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                              \n",
      " 27%|â–ˆâ–ˆâ–‹       | 4/15 [00:56<02:30, 13.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3959631323814392, 'eval_runtime': 2.2295, 'eval_samples_per_second': 2.243, 'eval_steps_per_second': 0.449, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                              \n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5/15 [01:10<02:16, 13.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3543678820133209, 'eval_runtime': 2.2975, 'eval_samples_per_second': 2.176, 'eval_steps_per_second': 0.435, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                              \n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6/15 [01:24<02:04, 13.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.32235580682754517, 'eval_runtime': 2.2644, 'eval_samples_per_second': 2.208, 'eval_steps_per_second': 0.442, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                              \n",
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 7/15 [01:38<01:49, 13.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2906649112701416, 'eval_runtime': 2.2386, 'eval_samples_per_second': 2.234, 'eval_steps_per_second': 0.447, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                              \n",
      " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 8/15 [01:51<01:35, 13.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.26951104402542114, 'eval_runtime': 2.2105, 'eval_samples_per_second': 2.262, 'eval_steps_per_second': 0.452, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                              \n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [02:04<01:21, 13.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.25194209814071655, 'eval_runtime': 2.2368, 'eval_samples_per_second': 2.235, 'eval_steps_per_second': 0.447, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [02:16<01:07, 13.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4209, 'grad_norm': 1.8948373794555664, 'learning_rate': 1.6666666666666667e-05, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [02:18<01:07, 13.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2332443743944168, 'eval_runtime': 2.2593, 'eval_samples_per_second': 2.213, 'eval_steps_per_second': 0.443, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11/15 [02:31<00:54, 13.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.21643559634685516, 'eval_runtime': 2.2956, 'eval_samples_per_second': 2.178, 'eval_steps_per_second': 0.436, 'epoch': 11.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12/15 [02:45<00:40, 13.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.20469920337200165, 'eval_runtime': 2.287, 'eval_samples_per_second': 2.186, 'eval_steps_per_second': 0.437, 'epoch': 12.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13/15 [02:59<00:27, 13.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1965031772851944, 'eval_runtime': 2.2601, 'eval_samples_per_second': 2.212, 'eval_steps_per_second': 0.442, 'epoch': 13.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 14/15 [03:12<00:13, 13.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1913081556558609, 'eval_runtime': 2.2512, 'eval_samples_per_second': 2.221, 'eval_steps_per_second': 0.444, 'epoch': 14.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [03:27<00:00, 13.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.18876613676548004, 'eval_runtime': 2.3921, 'eval_samples_per_second': 2.09, 'eval_steps_per_second': 0.418, 'epoch': 15.0}\n",
      "{'train_runtime': 207.5921, 'train_samples_per_second': 0.361, 'train_steps_per_second': 0.072, 'train_loss': 0.3560316562652588, 'epoch': 15.0}\n",
      "âœ… Model saved to ./wav2vec2_copyright\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import Wav2Vec2ForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "# âœ… Step 1: Define Custom Audio Dataset\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, audio_data, labels):\n",
    "        self.audio_data = audio_data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_values\": self.audio_data[idx],\n",
    "            \"labels\": self.labels[idx]\n",
    "        }\n",
    "\n",
    "# âœ… Step 2: Function to Load Wav2Vec2 Model\n",
    "def load_model(model_name=\"facebook/wav2vec2-base\"):\n",
    "    \"\"\"\n",
    "    Loads the Wav2Vec2 model for binary classification (copyright detection).\n",
    "    \"\"\"\n",
    "    model = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=2,\n",
    "        problem_type=\"single_label_classification\"\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# âœ… Step 3: Train Model on Full Dataset and Evaluate on Same Data\n",
    "def train_model(train_audio, train_labels, model_name=\"facebook/wav2vec2-base\", output_dir=\"./wav2vec2_copyright\"):\n",
    "    \"\"\"\n",
    "    Trains Wav2Vec2 on the provided dataset and evaluates on the same dataset.\n",
    "    \"\"\"\n",
    "    # Use full dataset for both training and evaluation\n",
    "    train_dataset = AudioDataset(train_audio, train_labels)\n",
    "    eval_dataset = train_dataset  # âœ… Evaluating on the same dataset\n",
    "\n",
    "    # Load model\n",
    "    model = load_model(model_name)\n",
    "\n",
    "    # Define Training Arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        per_device_train_batch_size=8,\n",
    "        learning_rate=5e-5,\n",
    "        num_train_epochs=15,\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_steps=10,\n",
    "        save_steps=500,\n",
    "        evaluation_strategy=\"epoch\"  # âœ… Evaluates after every epoch\n",
    "    )\n",
    "\n",
    "    # Initialize Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset  # âœ… Now evaluating on full training data\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    trainer.train()\n",
    "\n",
    "    # Save fine-tuned model\n",
    "    model.save_pretrained(output_dir)\n",
    "    print(f\"âœ… Model saved to {output_dir}\")\n",
    "\n",
    "train_model(train_audio, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Result: Copyrighted\n"
     ]
    }
   ],
   "source": [
    "def predict_audio(audio_path, model_path=\"./wav2vec2_copyright\"):\n",
    "    \"\"\"\n",
    "    Predicts whether an audio clip is copyrighted.\n",
    "    \"\"\"\n",
    "    # Load fine-tuned model\n",
    "    model = Wav2Vec2ForSequenceClassification.from_pretrained(model_path)\n",
    "    \n",
    "    # Load and process audio\n",
    "    audio = load_audio(audio_path)\n",
    "    inputs = processor(audio, return_tensors=\"pt\", padding=True, sampling_rate=16000)\n",
    "\n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        predicted_class = torch.argmax(logits).item()\n",
    "    \n",
    "    return \"Copyrighted\" if predicted_class == 1 else \"Not Copyrighted\"\n",
    "\n",
    "# Test with a new audio clip\n",
    "test_audio = \"dataset/test/test3.mp3\"\n",
    "result = predict_audio(test_audio)\n",
    "print(f\"ðŸ” Result: {result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
